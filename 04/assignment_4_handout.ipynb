{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Unsupervised Learning: Dimensionality Reduction, Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This assignment is worth a total of **10 points**. The goal of this assignment is to introduce you to clustering and dimensionality reduction techniques. \n",
    "\n",
    "We have structured the assignment into two major parts:\n",
    "\n",
    "1. **Part One**: Clustering technique\n",
    "2. **Part Two**: Dimensionality reduction (PCA)\n",
    "\n",
    "To install any unavailable package, you can do it similarly to the previous assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part One: Clustering,  <span style=\"color:green\">total of 6.5 points </span> \n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Necessary Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will work with the Digits dataset from scikit-learn, which consists of 8x8 pixel images of handwritten digits (0-9). \n",
    "Each datapoint is a 8x8 image of a digit.\n",
    "\n",
    "    =================   ==============\n",
    "    Classes                         10\n",
    "    Samples per class             ~180\n",
    "    Samples total                 1797\n",
    "    Dimensionality                  64\n",
    "    Features             integers 0-16\n",
    "    =================   =============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_digits()\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = data.data\n",
    "y = data.target  # True labels (for information only, not used later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some sample digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    random_index = np.random.randint(\n",
    "        0, len(X)\n",
    "    )  # Choose a random index from the training set\n",
    "    image = X[random_index].reshape(8, 8)  # Reshape the flat vector back to 8x8 image\n",
    "    label = y[random_index]  # Get the corresponding label\n",
    "    axes[i].imshow(image, cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Digit: {label}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal $k$ in the K-means cluster model\n",
    "\n",
    "We will try using the elbow heuristic ($k \\in [1,30]$) to determine the number of clusters in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_k = 30\n",
    "inertia = []  # List to store inertia values for each k\n",
    "# Calculate inertia for each number of clusters\n",
    "for k in range(1, upper_bound_k + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)  # Fit the model to the data\n",
    "    inertia.append(kmeans.inertia_)  # Store the inertia\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, upper_bound_k + 1), inertia, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "plt.title(\"Elbow Curve for k-means\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.xticks(range(1, upper_bound_k + 1))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 1: Answer the following questions</span>\n",
    "\n",
    "1. **Do clustering algorithms (like $k$-means) take into account the number of classes or the value of the labels in a dataset?**\n",
    "\n",
    "\n",
    "2. **How might variations in handwriting, such as different writing styles, pen types, or stroke patterns, affect the clustering of digits? Can a single class (e.g., the digit '8') appear in more than one cluster?**\n",
    "\n",
    "\n",
    "3. **Can one cluster contain data points from multiple classes (e.g., digits '5' and '8')?**\n",
    "\n",
    "\n",
    "4. **Is the elbow method an entirely objective way to determine the optimal number of clusters $k$?**\n",
    "\n",
    "\n",
    "#### <span style=\"color:green\">Total: 3 points </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering as a preprocessing step / semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering can also be used as a preprocessing step to help improve your classifier. For instance, in spatial data (e.g. data from GPS) it is possible to create new features by using the distance from the centroids. In this case, you use clustering to help with having limited data. We will work on the same Digits dataset from scikit-learn. \n",
    "\n",
    "    =================   ==============\n",
    "    Classes                         10\n",
    "    Samples per class             ~180\n",
    "    Samples total                 1797\n",
    "    Dimensionality                  64\n",
    "    Features             integers 0-16\n",
    "    =================   =============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.7, random_state=random_state\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a LogisticRegression model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_classifier = LogisticRegression(max_iter=10_000, random_state=random_state)\n",
    "initial_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy on the given test dataset:\\n {initial_classifier.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 2: Train a LogisticRegression model on a reduced version of the dataset</span>\n",
    "\n",
    "\n",
    "In this step, we will first train a Logistic Regression classifier on a smaller labeled sample of the Digits dataset. In typical machine learning scenarios, having a fully labeled dataset is crucial, but in some contexts, we may only have a small labeled subset, while the rest of the dataset remains unlabeled. This situation is commonly referred to as semi-supervised learning. Let's assume we only have the first 50 out of 1800 digits classified.\n",
    "\n",
    "#### <span style=\"color:green\">Total: 1 point </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = 50\n",
    "limited_classifier = LogisticRegression(max_iter=10_000)\n",
    "# TODO: fit the classifier using the first 50 data (use n_labeled)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy of the classifier using only {n_labeled} datapoints, on the given test dataset:\\n {limited_classifier.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 3: Training on the representative points</span>\n",
    "\n",
    "Now for the next experiment, we will use k-Means clustering to group the unlabeled data. After clustering, we will obtain the closest points to the cluster centroids. Then we will \"label\" these representative points and train a classifier. For a fair comparison, we will use 50 data points in this case too. \n",
    "\n",
    "(Note: K-medoids would have been a valid choice too)\n",
    "\n",
    "#### <span style=\"color:green\">Total: 1.25 points </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit Kmeans to the data\n",
    "k =   # TODO: number of clusters?\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# compute the distance from each point to each cluster centroid\n",
    "X_digits_distances = kmeans.transform(X_train)\n",
    "\n",
    "\n",
    "representative_digit_idx =  # TODO: # get the indices of the representative (closest) data points\n",
    "# get the data points represented by those indices\n",
    "X_representative_digits = X_train[representative_digit_idx]\n",
    "\n",
    "# get the class labels of the representative data points\n",
    "y_representative_digits = np.array( TODO )  # TODO: # get the class labels of the representative data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the representative digit images (one per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(\n",
    "        X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The labels of these representative digits are: \\n {y_representative_digits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the new representative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_log_reg = LogisticRegression(max_iter=10_000)\n",
    "cluster_log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy of the classifier using only {n_labeled} representative data points, on the given test dataset:\\n {cluster_log_reg.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 4: Let's use all the data points</span>\n",
    "We saw that even with 50 representative points we can get a performance boost. But what about the rest of data points? How could we automatically label the rest?\n",
    "\n",
    "One extra step we can take is propagating labels to the remaining unlabeled data points using the cluster structure weâ€™ve learned with KMeans. By assigning labels to the other data points based on the clusters they belong to, we can extend the labeled dataset without manually labeling every point.\n",
    "\n",
    "#### <span style=\"color:green\">Total: 1.25 points </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array to hold the new \"automatic\" labels\n",
    "y_train_automatic = np.empty(TODO, dtype=np.int64)  # TODO: size of array?\n",
    "\n",
    "# Assign to the empty array the correct values.\n",
    "# TODO: use kmeans.labels_ and the labels of the representative digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_log_reg = LogisticRegression(max_iter=10_000)\n",
    "automatic_log_reg.fit(X_train, y_train_automatic)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy using all data points: {initial_classifier.score(X_test, y_test):.4f}\",\n",
    "    f\"\\nAccuracy using the first 50 labelled data points: {limited_classifier.score(X_test, y_test):.4f}\",\n",
    "    f\"\\nAccuracy using the representative 50 data points: {cluster_log_reg.score(X_test, y_test):.4f}\"\n",
    "    f\"\\nAccuracy using all automatically labeled data points: {automatic_log_reg.score(X_test, y_test):.4f}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Dimensionality Reduction, <span style=\"color:green\">total of 3.5 points </span> \n",
    "\n",
    "\n",
    "\n",
    "In the second part of this assignment, you will apply PCA for dimensionality reduction and then followed by a classification task. As you may have already seen in the lecture, PCA is a good technique that allows us find the k-dimensional representation of the data that maximizes the variance of the data. As such it is possible to reduce the dimension of the data while still maintianing the most important properties of the sample. We will still work with the digits datasets used in part one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the Digits dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.125, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 5: You will need to complete this function by performing PCA and followed by logistic regression</span>\n",
    "\n",
    "#### <span style=\"color:green\">Total: 2.5 points </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_pca_accuracy(X_train, Y_train, X_val, Y_val, n_components_list):\n",
    "    \"\"\"Evaluate accuracy of Logistic Regression with different numbers of PCA components.\"\"\"\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for n_components in n_components_list:\n",
    "        #TODO: Apply PCA\n",
    "        pca = #TODO: Apply PCA\n",
    "        X_train_pca = #TODO: Apply PCA to train\n",
    "        X_val_pca = #TODO: Apply PCA to val\n",
    "\n",
    "        \n",
    "        #TODO: Train Logistic Regression model\n",
    "        clf = #TODO: Train Logistic Regression model\n",
    "        \n",
    "        train_pred =   #TODO: predictions\n",
    "        val_pred =   #TODO: predictions\n",
    "\n",
    "        train_accuracy = #TODO: Evaluate accuracy\n",
    "        val_accuracy = #TODO: Evaluate accuracy\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    return   #TODO: accuracies for train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_list = [2, 5, 10, 20, 30, 40, 50, 60]\n",
    "train_accuracies, val_accuracies = evaluate_pca_accuracy(\n",
    "    X_train, Y_train, X_val, Y_val, n_components_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "# TODO:Plot the  training accuracies with marker = 'o'\n",
    "# TODO:Plot the  validation accuracies with marker = 'o'\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of PCA on Logistic Regression Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 6: Select the optimal number of PCA components based on the plot and justify your chocie in terms of model complexity.\n",
    " </span>\n",
    " \n",
    " \n",
    "#### <span style=\"color:green\">Total: 1 point </span> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beyondbinarydecisions)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

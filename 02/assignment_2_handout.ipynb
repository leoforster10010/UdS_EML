{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Classification\n",
    "\n",
    "Submitted by:\n",
    "- Jonas Henker, 7054995\n",
    "- Leo Forster, 7055800\n",
    "\n",
    "In this assigment you will be coding for a Classification task hands-on. (10 Points) Fill the part with #TODO\n",
    "\n",
    "This notebook will guide you through the process of using logistic regression, LDA and QDA for classification tasks. You will start with a binary classification problem and attempt to solve it using logistic regression. You will then extend it to a more complicated dataset to classify it using LDA and QDA\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "- Implement logistic regression for binary classification.\n",
    "- Understand differences between LDA and QDA.\n",
    "- Implement LDA and QDA for multivariate data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:08:26.520516Z",
     "start_time": "2024-11-27T10:08:25.666532Z"
    }
   },
   "source": [
    "## Import Necessary Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "cmap = colors.ListedColormap([\"tab:red\", \"tab:blue\"])\n",
    "\n",
    "def plot_ellipse(mean, cov, color, ax):\n",
    "    v, w = np.linalg.eigh(cov)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(\n",
    "        mean,\n",
    "        2 * v[0] ** 0.5,\n",
    "        2 * v[1] ** 0.5,\n",
    "        angle=180 + angle,\n",
    "        facecolor=color,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ell.set_clip_box(ax.bbox)\n",
    "    ell.set_alpha(0.4)\n",
    "    ax.add_artist(ell)\n",
    "\n",
    "\n",
    "def plot_result(estimator, X, y, ax):\n",
    "    cmap = colors.ListedColormap([\"tab:red\", \"tab:blue\"])\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator,\n",
    "        X,\n",
    "        response_method=\"predict_proba\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator,\n",
    "        X,\n",
    "        response_method=\"predict_proba\",\n",
    "        plot_method=\"contour\",\n",
    "        ax=ax,\n",
    "        alpha=1.0,\n",
    "        levels=[0.5],\n",
    "    )\n",
    "    y_pred = estimator.predict(X)\n",
    "    X_right, y_right = X[y == y_pred], y[y == y_pred]\n",
    "    X_wrong, y_wrong = X[y != y_pred], y[y != y_pred]\n",
    "    ax.scatter(X_right[:, 0], X_right[:, 1], c=y_right, s=20, cmap=cmap, alpha=0.5)\n",
    "    ax.scatter(\n",
    "        X_wrong[:, 0],\n",
    "        X_wrong[:, 1],\n",
    "        c=y_wrong,\n",
    "        s=30,\n",
    "        cmap=cmap,\n",
    "        alpha=0.9,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        estimator.means_[:, 0],\n",
    "        estimator.means_[:, 1],\n",
    "        c=\"yellow\",\n",
    "        s=200,\n",
    "        marker=\"*\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    if isinstance(estimator, LinearDiscriminantAnalysis):\n",
    "        covariance = [estimator.covariance_] * 2\n",
    "    else:\n",
    "        covariance = estimator.covariance_\n",
    "    plot_ellipse(estimator.means_[0], covariance[0], \"tab:red\", ax)\n",
    "    plot_ellipse(estimator.means_[1], covariance[1], \"tab:blue\", ax)\n",
    "\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set(xticks=[], yticks=[])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression for Classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=250, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, n_clusters_per_class=1, random_state=1)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(12,12))\n",
    "# Plot points for Class 0\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=cmap(0.1), s=50, label='Class 0', alpha=0.6, edgecolors='k')\n",
    "# Plot points for Class 1\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=cmap(0.9), s=50, label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(['Class 0', 'Class 1'])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Task 1: Check range of outputs by plotting the histogram of them comment on the reason of the interval.</span> (2 Points)\n",
    "Answer: The peaks at 0 & 1 show that the model ist mostly confident about its classifications. But the values around 0.6 could suggest, that more training or a more complex model is needed, to classify these values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "logistic_regressor = LogisticRegression()\n",
    "logistic_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_all = logistic_regressor.predict_proba(X_test)\n",
    "\n",
    "# Select only the probabilities for class 1\n",
    "y_pred_prob = y_pred_prob_all[:, 1]\n",
    "\n",
    "# TODO - Print histogram of predicted probabilities\n",
    "plt.hist(y_pred_prob, bins=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of class 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Task 2: Convert the probability predictions to binary class labels using 0.5 as threshold and compute Type-1 and Type-2 error.</span> (2 Points)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO - Your code here\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f'Accuracy of Linear Regression Classifier: {accuracy:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "FP = cm[0, 1]  # False Positive\n",
    "FN = cm[1, 0]  # False Negative\n",
    "TN = cm[0, 0]  # True Negative\n",
    "TP = cm[1, 1]  # True Positive\n",
    "\n",
    "# TODO - Your code here # Calculate Type I and Type II error rates\n",
    "type1_error_rate = FP / (FP + TN)\n",
    "type2_error_rate = FN / (FN + TP)\n",
    "\n",
    "print(f'Type I Error Rate (False Positive Rate): {type1_error_rate:.2f}')\n",
    "print(f'Type II Error Rate (False Negative Rate): {type2_error_rate:.2f}')\n",
    "\n",
    "# Visualize the confusion matrix with heatmap\n",
    "cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Task 3: Determine correct predictions.</span> (1 Point)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO - Your code here\n",
    "X_right = X_test[y_test == y_pred_class]\n",
    "y_right = y_test[y_test == y_pred_class]\n",
    "X_wrong = X_test[y_test != y_pred_class]\n",
    "y_wrong = y_test[y_test != y_pred_class]\n",
    "\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "# Define the mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                     np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Predict using logistic regression on the mesh grid\n",
    "Z = logistic_regressor.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "plt.scatter(X_right[:, 0], X_right[:, 1], c=y_right, cmap='coolwarm', alpha=0.9, label='Correctly classified', edgecolors='k')\n",
    "plt.scatter(\n",
    "        X_wrong[:, 0],\n",
    "        X_wrong[:, 1],\n",
    "        c=y_wrong,\n",
    "        cmap='coolwarm',\n",
    "        alpha=0.9,\n",
    "        marker=\"x\",\n",
    "        label='Misclassified'\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Gaussian Distributed Data\n",
    "\n",
    "Below we generate three datasets with different Gaussian distributions.\n",
    "\n",
    "First, we define a function to generate synthetic data. It creates two blobs centered\n",
    "at `(0, 0)` and `(1, 1)`. Each blob is assigned a specific class. The dispersion of\n",
    "the blob is controlled by the parameters `cov_class_1` and `cov_class_2`, that are the\n",
    "covariance matrices used when generating the samples from the Gaussian distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def make_data(n_samples, n_features, cov_class_1, cov_class_2, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = np.concatenate(\n",
    "        [\n",
    "            rng.randn(n_samples, n_features) @ cov_class_1,\n",
    "            rng.randn(n_samples, n_features) @ cov_class_2 + np.array([1, 1]),\n",
    "        ]\n",
    "    )\n",
    "    y = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "covariance = np.array([[1, 0], [0, 1]])\n",
    "X_1, y_1 = make_data(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    cov_class_1=covariance,\n",
    "    cov_class_2=covariance,\n",
    "    seed=0,\n",
    ")\n",
    "covariance = np.array([[0.0, -0.23], [0.83, 0.23]])\n",
    "X_2, y_2 = make_data(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    cov_class_1=covariance,\n",
    "    cov_class_2=covariance,\n",
    "    seed=0,\n",
    ")\n",
    "cov_class_1 = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0\n",
    "cov_class_2 = cov_class_1.T\n",
    "X_3, y_3 = make_data(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    cov_class_1=cov_class_1,\n",
    "    cov_class_2=cov_class_2,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "#PLOT DATASETS\n",
    "plt.figure(figsize=(6, 6))\n",
    "print(X_1.shape, y_1.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], c=y_1, cmap='coolwarm', edgecolors='k', alpha=0.3)\n",
    "plt.title('Scenerio 1')#('Scenerio Covariance')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], c=y_2, cmap='coolwarm', edgecolors='k', alpha=0.6)\n",
    "plt.title('Scenerio 2')#('Shared Covariance')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3, cmap='coolwarm', edgecolors='k',alpha=0.6)\n",
    "plt.title('Scenerio 3')#('Different Covariance')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Task 4: Match the plotted scenerios with the following cases </span> (1 Points)\n",
    "- Shared Covariance: Scenario 2\n",
    "- Different Covariance: Scenario 3\n",
    "- Isotropic Covariance: Scenario 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember LDA and QDA:\n",
    "\n",
    "- **Multivariate LDA**:\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) \\propto \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma^{-1} (\\mathbf{x} - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "- **Multivariate LDA Decision Function**:\n",
    "  $$\n",
    "  \\delta_k(\\mathbf{x}) = \\mathbf{x}^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log P(y = k)\n",
    "  $$\n",
    "  - $ \\mu_k $ is the mean vector for class $ k $,\n",
    "  - $ \\Sigma $ is the shared matrix ,\n",
    "  - $ |\\Sigma| $ is the determinant of $ \\Sigma $,\n",
    "  - $ P(y = k) $ is the prior probability for class $ k $.\n",
    "  - Assumes $ \\Sigma_1 = \\Sigma_2 = \\dots = \\Sigma_k $. \n",
    "<br /><br /><br /><br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Multivariate QDA**:\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) \\propto \\frac{1}{|\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- **QDA Decision Function**:\n",
    "  $$\n",
    "  \\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k) + \\log P(y = k)\n",
    "  $$\n",
    "  - $ \\mu_k $ is the mean vector for class $ k $,\n",
    "  - $ \\Sigma_k $ is the covariance matrix for class $ k $,\n",
    "  - $ |\\Sigma_k| $ is the determinant of $ \\Sigma_k $,\n",
    "  - $ P(y = k) $ is the prior probability for class $ k $.\n",
    "  - Assumes $ \\Sigma_1 \\neq \\Sigma_2 \\neq \\dots \\neq \\Sigma_k \\ $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Task 5: Assume that we need to perform binary classification for plotted Scenarios 1, 2, and 3. Choose one classification method (QDA or LDA) to use in each scenario. If you believe both methods are suitable, always select the simpler one. </span> (2 Points)\n",
    "- Scenerio 1: LDA\n",
    "- Scenerio 2: LDA\n",
    "- Scenerio 3: QDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the performance of LDA and QDA for each case"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, sharex=\"row\", sharey=\"row\", figsize=(16,20))\n",
    "lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "for ax_row, X, y in zip(\n",
    "    axs,\n",
    "    (X_1, X_2, X_3),\n",
    "    (y_1, y_2, y_3),\n",
    "):\n",
    "    lda.fit(X, y)\n",
    "    plot_result(lda, X, y, ax_row[0])\n",
    "    qda.fit(X, y)\n",
    "    plot_result(qda, X, y, ax_row[1])\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Scenerio 1\", fontsize=15)\n",
    "axs[1, 0].set_ylabel(\"Scenerio 2\", fontsize=15)\n",
    "axs[2, 0].set_ylabel(\"Scenerio 3\", fontsize=15)\n",
    "fig.suptitle(\n",
    "    \"Linear Discriminant Analysis (Left) vs Quadratic Discriminant Analysis (Right)\",\n",
    "    y=0.94,\n",
    "    fontsize=15,\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Task 6: For each of the scenerios plotted above, compare behaviour of decision boundaries with your words for LDA (left column) and QDA (right column). </span> (2 Points)\n",
    "\n",
    "- Scenerio 1: In both cases, the decision boundaries are linear, thus using LDA is more fitting, given the lower complexity\n",
    "- Scenerio 2: In both cases, the decision boundaries are linear or near linear, thus using LDA is more fitting, given the lower complexity\n",
    "- Scenerio 3: In this scenario, the quadratic boundaries of QDA fit the data better, thus QDA is the better choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
